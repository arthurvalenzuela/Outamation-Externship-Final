{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMTC+CulhqP3xjhju3vE1zU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arthurvalenzuela/Outamation-Externship-Final/blob/main/RAG_Pipeline_%22Lucky%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 1: Install Required Libraries\n",
        "\n",
        "This cell installs all the necessary software packages for the project using pip. It includes libraries for handling PDFs (PyMuPDF, Surya), running AI models (torch), and building the main question-answering system (LlamaIndex and its specific integrations for embeddings, vector stores, LLMs, and utilities). It also checks the installed version of the core llama-index library.\n",
        "\n"
      ],
      "metadata": {
        "id": "JzSgJn7w_LRo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcDHoQvtBlSr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "971b9937-aa62-4c83-c0a2-80062bc7770e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m154.1/154.1 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m117.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.1/253.1 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.2/129.2 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m306.7/306.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h--- Library Installations Complete ---\n",
            "\n",
            "--- Checking llama-index version ---\n",
            "Name: llama-index\n",
            "Version: 0.12.30\n",
            "Summary: Interface between LLMs and your data\n",
            "Home-page: https://llamaindex.ai\n",
            "Author: Jerry Liu\n",
            "Author-email: jerry@llamaindex.ai\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: llama-index-agent-openai, llama-index-cli, llama-index-core, llama-index-embeddings-openai, llama-index-indices-managed-llama-cloud, llama-index-llms-openai, llama-index-multi-modal-llms-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index-readers-file, llama-index-readers-llama-parse, nltk\n",
            "Required-by: \n",
            "--- ---\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Install Required Libraries\n",
        "!pip install PyMuPDF surya-ocr torch --quiet\n",
        "# Install core LlamaIndex, integrations, and supporting libraries\n",
        "!pip install llama-index llama-index-embeddings-huggingface sentence-transformers --quiet\n",
        "# Install Qdrant and Groq integrations separately\n",
        "!pip install llama-index-vector-stores-qdrant qdrant-client llama-index-llms-groq --quiet\n",
        "\n",
        "print(\"--- Library Installations Complete ---\")\n",
        "print(\"\\n--- Checking llama-index version ---\")\n",
        "!pip show llama-index # Displays the installed version for reference\n",
        "print(\"--- ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 2: Import Libraries into Notebook\n",
        "\n",
        " This cell imports the specific functions, classes, and modules from the previously installed libraries that will be used throughout the notebook. Grouping imports here helps keep track of dependencies."
      ],
      "metadata": {
        "id": "63SkGy6C_Pml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Import Libraries into Notebook\n",
        "# Standard libraries\n",
        "import os\n",
        "import time # For timing queries and pauses\n",
        "import numpy as np\n",
        "from PIL import Image # For handling images during PDF processing\n",
        "\n",
        "# PDF processing libraries\n",
        "import fitz  # PyMuPDF for opening PDFs\n",
        "from surya.detection import DetectionPredictor # Surya for finding text blocks\n",
        "from surya.recognition import RecognitionPredictor # Surya for reading text (OCR)\n",
        "\n",
        "# Google Colab specific imports\n",
        "from google.colab import files # For uploading files\n",
        "from google.colab import userdata # For accessing API keys securely\n",
        "from IPython.display import display, Markdown # For formatted output\n",
        "\n",
        "# LlamaIndex and RAG component imports\n",
        "from llama_index.core import VectorStoreIndex, StorageContext, Document as LlamaDocument\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.settings import Settings\n",
        "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
        "from llama_index.core import get_response_synthesizer\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "from llama_index.llms.groq import Groq\n",
        "\n",
        "# Qdrant client library and models\n",
        "import qdrant_client\n",
        "from qdrant_client import models as qdrant_models # Import the models submodule\n",
        "\n",
        "print(\"Required libraries imported.\")"
      ],
      "metadata": {
        "id": "20QUJtQrFrsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 3: Upload PDF Files\n",
        "\n",
        "This cell uses Google Colab's file upload feature to allow you to select and upload the PDF documents you want the system to process. It supports uploading multiple files at once and stores their names for later use."
      ],
      "metadata": {
        "id": "t_0HZszP_ixT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Upload PDF Files\n",
        "import os # Ensure os is imported\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Please upload ALL your PDF files for the RAG pipeline:\")\n",
        "# This will open the file upload dialog where you can select multiple PDFs\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "pdf_filenames = [] # Initialize the LIST here\n",
        "if uploaded_files:\n",
        "    pdf_filenames = list(uploaded_files.keys()) # Get the list of filenames\n",
        "    print(f\"\\nUploaded {len(pdf_filenames)} files:\")\n",
        "    for name in pdf_filenames:\n",
        "      print(f\"- {name}\")\n",
        "    # Optional: Check files in the current directory\n",
        "    # print(\"\\nFiles in current directory:\", os.listdir())\n",
        "else:\n",
        "    print(\"No files uploaded. Please run this cell again and upload files.\")\n",
        "\n",
        "# The 'pdf_filenames' list now holds the names of files ready for processing in Cell 5"
      ],
      "metadata": {
        "id": "3TolxkIRF5NQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 4: Configure PDF Processing Settings\n",
        "\n",
        "This cell sets up important configuration options for processing the PDFs in the next step. It defines the language for the OCR model (Surya) and the image resolution (DPI) used when converting PDF pages to images for analysis. It also initializes the Surya AI models needed for reading the text."
      ],
      "metadata": {
        "id": "VHC3Q9Ue_lrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Configure PDF Processing Settings\n",
        "import os # Ensure os is imported\n",
        "# Import Surya classes if not already done in Cell 2\n",
        "from surya.detection import DetectionPredictor\n",
        "from surya.recognition import RecognitionPredictor\n",
        "\n",
        "# Try to speed up Surya using compilation (optional, may add overhead)\n",
        "# os.environ['COMPILE_ALL'] = 'true'\n",
        "# print(f\"Attempting to enable Surya compilation...\")\n",
        "\n",
        "# Define language(s) for OCR - Helps Surya recognize text better\n",
        "LANGUAGES = [\"en\"] # Set to the primary language of your documents\n",
        "\n",
        "# Set DPI (Dots Per Inch) for rendering PDF pages as images for OCR.\n",
        "# Lower DPI is faster and uses less memory but might miss small text.\n",
        "RENDERING_DPI = 96 # Using 96 for better memory efficiency\n",
        "\n",
        "# Initialize Surya OCR models\n",
        "det_predictor = None\n",
        "rec_predictor = None\n",
        "surya_ready = False # Flag to check initialization\n",
        "try:\n",
        "    print(\"Initializing Surya text detection and recognition models...\")\n",
        "    det_predictor = DetectionPredictor()\n",
        "    rec_predictor = RecognitionPredictor()\n",
        "    print(\"Surya models initialized.\")\n",
        "    surya_ready = True\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing Surya models: {e}\")\n",
        "    surya_ready = False\n",
        "\n",
        "if surya_ready:\n",
        "    print(\"Configuration set and Surya models ready.\")\n",
        "else:\n",
        "    print(\"Surya models failed to initialize. PDF processing cell (Cell 5) may fail.\")"
      ],
      "metadata": {
        "id": "aOvcGmfrF8Xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 5: Process PDFs and Create Text Chunks\n",
        "\n",
        "This crucial cell performs the core data processing. It loops through each PDF uploaded in Cell 3, uses the initialized Surya models (from Cell 4) to extract text page-by-page, combines the text for each document, and then splits this text into smaller, overlapping chunks (called \"Nodes\"). These nodes, along with their source filename, are stored for indexing in the next steps.\n",
        "\n"
      ],
      "metadata": {
        "id": "djB1p6CO_q_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Process PDFs and Create Text Chunks\n",
        "# Ensure necessary imports are available\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.schema import Document as LlamaDocument\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import fitz\n",
        "\n",
        "# Define batch size for Surya processing (how many pages to process at once)\n",
        "# BATCH_SIZE = 1 was found to work best for memory constraints in previous tests\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "# --- RAG Settings ---\n",
        "CHUNK_SIZE = 512 # Target size for each text chunk\n",
        "CHUNK_OVERLAP = 50  # How much text overlaps between chunks\n",
        "\n",
        "# Initialize list to store LlamaIndex Node objects (text chunks)\n",
        "all_nodes = []\n",
        "\n",
        "# Check if files were uploaded (Cell 3) and Surya is ready (Cell 4)\n",
        "if ('pdf_filenames' in locals() and pdf_filenames and\n",
        "    'surya_ready' in locals() and surya_ready and\n",
        "    det_predictor and rec_predictor):\n",
        "\n",
        "    print(f\"\\nStarting PDF processing and chunking...\")\n",
        "    # Initialize the text splitter\n",
        "    node_parser = SentenceSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
        "\n",
        "    # Loop through each uploaded PDF filename\n",
        "    for pdf_filename in pdf_filenames:\n",
        "        print(f\"\\n===== Processing PDF: {pdf_filename} =====\")\n",
        "        doc_fitz = None # Initialize variable\n",
        "        try:\n",
        "            doc_fitz = fitz.open(pdf_filename) # Open the PDF file\n",
        "            num_pages = doc_fitz.page_count\n",
        "            doc_text_content = \"\" # Accumulate text for this document\n",
        "\n",
        "            # Process pages in batches\n",
        "            for i in range(0, num_pages, BATCH_SIZE):\n",
        "                batch_indices = range(i, min(i + BATCH_SIZE, num_pages))\n",
        "                print(f\"--- Processing Batch: Pages {batch_indices.start + 1} to {batch_indices.stop} ---\")\n",
        "\n",
        "                batch_pil_images = []\n",
        "                page_numbers_in_batch = []\n",
        "\n",
        "                # 1. Render pages in batch to images\n",
        "                for page_num_idx in batch_indices:\n",
        "                    try:\n",
        "                        page = doc_fitz[page_num_idx]\n",
        "                        pix = page.get_pixmap(dpi=RENDERING_DPI)\n",
        "                        img_pil = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
        "                        batch_pil_images.append(img_pil)\n",
        "                        page_numbers_in_batch.append(page_num_idx + 1)\n",
        "                    except Exception as e:\n",
        "                         print(f\"  Error rendering page {page_num_idx + 1} in {pdf_filename}: {e}\")\n",
        "                         continue\n",
        "\n",
        "                if not batch_pil_images: continue\n",
        "\n",
        "                # 2. Run Surya OCR prediction\n",
        "                try:\n",
        "                    batch_langs = [LANGUAGES] * len(batch_pil_images)\n",
        "                    batch_predictions = rec_predictor(batch_pil_images, batch_langs, det_predictor)\n",
        "                except Exception as e:\n",
        "                    print(f\"  Error during Surya prediction for batch starting at page {i+1} in {pdf_filename}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                # 3. Extract recognized text lines\n",
        "                for idx, page_prediction in enumerate(batch_predictions):\n",
        "                    page_num_actual = page_numbers_in_batch[idx]\n",
        "                    current_page_lines = []\n",
        "                    if hasattr(page_prediction, 'text_lines') and page_prediction.text_lines is not None:\n",
        "                         current_page_lines = page_prediction.text_lines\n",
        "                    for line in current_page_lines:\n",
        "                         if hasattr(line, 'text'):\n",
        "                             doc_text_content += line.text + \"\\n\"\n",
        "\n",
        "            # --- Chunking the extracted text for the document ---\n",
        "            if doc_text_content:\n",
        "                print(f\"  Chunking text for {pdf_filename}...\")\n",
        "                llama_doc = LlamaDocument(\n",
        "                    text=doc_text_content,\n",
        "                    metadata={\"file_name\": pdf_filename}\n",
        "                )\n",
        "                nodes = node_parser.get_nodes_from_documents([llama_doc])\n",
        "                print(f\"  Generated {len(nodes)} nodes (chunks) for {pdf_filename}.\")\n",
        "                all_nodes.extend(nodes) # Add chunks to the main list\n",
        "            else:\n",
        "                print(f\"  No text content extracted or processed for {pdf_filename}.\")\n",
        "\n",
        "            doc_fitz.close() # Close the PDF file\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to process PDF {pdf_filename}: {e}\")\n",
        "            if doc_fitz and not doc_fitz.is_closed: # Ensure closure on error\n",
        "                 doc_fitz.close()\n",
        "\n",
        "    print(f\"\\nFinished processing all PDFs. Total nodes created: {len(all_nodes)}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping PDF processing - Check if files were uploaded in Cell 3 and Surya models initialized in Cell 4.\")"
      ],
      "metadata": {
        "id": "s4I3EUXMGLbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 6: Configure API Keys\n",
        "\n",
        "This cell retrieves the necessary API keys you stored securely in Colab's Secrets Manager (accessible via the key icon ğŸ”‘ in the left sidebar). These keys are essential for connecting to external services used by the pipeline. Make sure you have added the required secrets before running this cell."
      ],
      "metadata": {
        "id": "-I_ym1tz_1BK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Configure API Keys\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# --- Secrets Needed (Add via Key Icon ğŸ”‘) ---\n",
        "#   - HUGGINGFACE_API_KEY (Your Hugging Face token)\n",
        "#   - QDRANT_URL         (Your Qdrant instance URL)\n",
        "#   - QDRANT_API_KEY      (Your Qdrant API key)\n",
        "#   - GROQ_API_KEY        (Your Groq API key)\n",
        "# --- Make sure notebook access is enabled for each secret. ---\n",
        "\n",
        "print(\"Retrieving API Keys from Colab Secrets...\")\n",
        "# Load keys into environment variables for libraries to use\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = userdata.get('HUGGINGFACE_API_KEY')\n",
        "os.environ[\"QDRANT_API_KEY\"] = userdata.get('QDRANT_API_KEY')\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "QDRANT_URL = userdata.get('QDRANT_URL') # Store Qdrant URL separately\n",
        "\n",
        "# Verification checks\n",
        "key_check_passed = True\n",
        "print(f\"- Checking HUGGINGFACE_API_KEY: {'Found' if os.environ.get('HUGGINGFACE_API_KEY') else '!!! MISSING !!!'}\")\n",
        "if not os.environ.get(\"HUGGINGFACE_API_KEY\"): key_check_passed = False\n",
        "print(f\"- Checking QDRANT_API_KEY: {'Found' if os.environ.get('QDRANT_API_KEY') else '!!! MISSING !!!'}\")\n",
        "if not os.environ.get(\"QDRANT_API_KEY\"): key_check_passed = False\n",
        "print(f\"- Checking GROQ_API_KEY: {'Found' if os.environ.get('GROQ_API_KEY') else '!!! MISSING !!!'}\")\n",
        "if not os.environ.get(\"GROQ_API_KEY\"): key_check_passed = False\n",
        "print(f\"- Checking QDRANT_URL: {'Found' if QDRANT_URL else '!!! MISSING !!!'}\")\n",
        "if not QDRANT_URL: key_check_passed = False\n",
        "\n",
        "if key_check_passed:\n",
        "    print(\"\\nRequired API Keys and Qdrant URL retrieved successfully.\")\n",
        "else:\n",
        "    print(\"\\nOne or more required API keys/URLs missing. Please check Colab Secrets (ğŸ”‘). Downstream cells might fail.\")"
      ],
      "metadata": {
        "id": "Zrkh_3-8e8wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 7: Setup Embedding Model and Vector Database Connection\n",
        "\n",
        "This cell initializes two key components:\n",
        "\n",
        "The Embedding Model (BAAI/bge-base-en-v1.5 from Hugging Face), which converts text chunks into numerical representations (vectors).\n",
        "The Vector Database Client (Qdrant), which connects to the service where these vectors and the associated text will be stored and searched. It also prepares the StorageContext needed by LlamaIndex."
      ],
      "metadata": {
        "id": "DC5WzpLW_92B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Setup Embedding Model and Vector Database Connection\n",
        "import os\n",
        "import qdrant_client\n",
        "# Ensure LlamaIndex imports are available\n",
        "from llama_index.core import StorageContext\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "from llama_index.core.settings import Settings\n",
        "# Import Qdrant models from Cell 2 or here\n",
        "from qdrant_client import models as qdrant_models\n",
        "\n",
        "# --- Initialize Embedding Model ---\n",
        "print(\"Initializing embedding model (BAAI/bge-base-en-v1.5)...\")\n",
        "embed_model_ready = False\n",
        "embed_model = None\n",
        "if os.environ.get(\"HUGGINGFACE_API_KEY\"):\n",
        "    try:\n",
        "        embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
        "        print(\"Embedding model initialized.\")\n",
        "        # Set this model as the default for LlamaIndex\n",
        "        Settings.embed_model = embed_model\n",
        "        print(\"Global Settings.embed_model set.\")\n",
        "        embed_model_ready = True\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing embedding model: {e}\")\n",
        "else:\n",
        "    print(\"HUGGINGFACE_API_KEY not found. Cannot initialize embedding model.\")\n",
        "\n",
        "# --- Initialize Qdrant Client and Vector Store ---\n",
        "print(\"Initializing Qdrant client...\")\n",
        "qdrant_ready = False\n",
        "storage_context = None\n",
        "vector_store = None\n",
        "# Define the name for the data collection within Qdrant\n",
        "# Using a distinct name helps avoid conflicts if re-running\n",
        "qdrant_collection_name = \"rag_pipeline_final_v1\"\n",
        "\n",
        "if 'QDRANT_URL' in locals() and QDRANT_URL and os.environ.get(\"QDRANT_API_KEY\"):\n",
        "    try:\n",
        "        # Connect to the Qdrant instance\n",
        "        qdrant_client_instance = qdrant_client.QdrantClient(\n",
        "            url=QDRANT_URL,\n",
        "            api_key=os.environ.get(\"QDRANT_API_KEY\"),\n",
        "            timeout=60\n",
        "        )\n",
        "        qdrant_client_instance.get_collections() # Verify connection\n",
        "        print(\"Qdrant client initialized and connection verified.\")\n",
        "\n",
        "        # Prepare the vector store object pointing to the specific collection\n",
        "        vector_store = QdrantVectorStore(\n",
        "            client=qdrant_client_instance,\n",
        "            collection_name=qdrant_collection_name,\n",
        "        )\n",
        "        # Prepare the storage context for LlamaIndex\n",
        "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "        print(f\"Qdrant vector store ('{qdrant_collection_name}') and storage context ready.\")\n",
        "        qdrant_ready = True\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing Qdrant client or vector store: {e}\")\n",
        "        print(\"Check QDRANT_URL, QDRANT_API_KEY, and ensure the Qdrant instance is running/accessible.\")\n",
        "else:\n",
        "    print(\"QDRANT_URL or QDRANT_API_KEY not found. Cannot initialize Qdrant.\")\n",
        "\n",
        "# Final status message\n",
        "if embed_model_ready and qdrant_ready:\n",
        "    print(\"\\nEmbedding model and Qdrant setup complete.\")\n",
        "else:\n",
        "    print(\"\\nSetup failed for Embedding model and/or Qdrant. Subsequent cells will likely fail.\")"
      ],
      "metadata": {
        "id": "Ua4We3csGQwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 8: Create/Reset the Index in Vector Database\n",
        "\n",
        "This cell builds the core knowledge base for the RAG system. It first attempts to reset (delete and recreate) the specified collection in the Qdrant database to ensure only the latest documents are included. Then, it takes the text chunks (Nodes) prepared in Cell 5, uses the embedding model (from Cell 7) to convert them into vectors, and stores these vectors along with the original text chunks in the Qdrant collection."
      ],
      "metadata": {
        "id": "aqw9jJF6AGJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Create/Reset the Index in Vector Database\n",
        "import time\n",
        "# Ensure necessary imports are available\n",
        "from llama_index.core import VectorStoreIndex, StorageContext\n",
        "from llama_index.core.settings import Settings\n",
        "import qdrant_client\n",
        "# Use the alias defined in Cell 2 or import here\n",
        "from qdrant_client import models as qdrant_models\n",
        "\n",
        "index = None # Initialize index variable\n",
        "\n",
        "print(\"\\n--- Entering Cell 8: Index Creation/Reset ---\")\n",
        "# Check prerequisites\n",
        "print(f\"Checking prerequisites: qdrant_ready={locals().get('qdrant_ready')}, embed_model_ready={locals().get('embed_model_ready')}\")\n",
        "print(f\"Is storage_context valid? {'Yes' if locals().get('storage_context') else 'No'}\")\n",
        "print(f\"Is embed_model valid? {'Yes' if locals().get('embed_model') else 'No'}\")\n",
        "\n",
        "if (qdrant_ready and embed_model_ready and storage_context and embed_model):\n",
        "\n",
        "    # Ensure global embed model is set\n",
        "    if Settings.embed_model is None: Settings.embed_model = embed_model\n",
        "\n",
        "    # --- Reset Qdrant Collection ---\n",
        "    collection_reset = False\n",
        "    if 'qdrant_client_instance' in locals() and qdrant_client_instance and 'qdrant_collection_name' in locals():\n",
        "        print(f\"Attempting to reset Qdrant collection: '{qdrant_collection_name}'...\")\n",
        "        try:\n",
        "            # Check existence before delete attempt\n",
        "            try:\n",
        "                qdrant_client_instance.get_collection(collection_name=qdrant_collection_name)\n",
        "                print(f\"  Collection '{qdrant_collection_name}' found. Deleting...\")\n",
        "                qdrant_client_instance.delete_collection(collection_name=qdrant_collection_name)\n",
        "                time.sleep(2)\n",
        "                print(f\"  Collection '{qdrant_collection_name}' deleted.\")\n",
        "            except Exception as e:\n",
        "                if \"doesn't exist\" in str(e) or \"Not found\" in str(e) or \"404\" in str(e):\n",
        "                   print(f\"  Collection '{qdrant_collection_name}' does not exist yet. Skipping deletion.\")\n",
        "                else:\n",
        "                   print(f\"  Warning during collection check/delete: {e}\")\n",
        "\n",
        "            # Re-create the collection\n",
        "            vector_size = 768 # Dimension for BAAI/bge-base-en-v1.5\n",
        "            print(f\"  Re-creating collection '{qdrant_collection_name}' with vector size: {vector_size}...\")\n",
        "            qdrant_client_instance.recreate_collection(\n",
        "                 collection_name=qdrant_collection_name,\n",
        "                 vectors_config=qdrant_models.VectorParams(size=vector_size, distance=qdrant_models.Distance.COSINE)\n",
        "            )\n",
        "            time.sleep(1)\n",
        "            print(f\"  Collection '{qdrant_collection_name}' recreated successfully.\")\n",
        "            collection_reset = True\n",
        "        except Exception as e:\n",
        "            print(f\"  Error resetting Qdrant collection: {e}\")\n",
        "            collection_reset = False\n",
        "    else:\n",
        "        print(\"  Qdrant client or collection name missing. Cannot reset collection.\")\n",
        "        collection_reset = False\n",
        "    # --- End Reset ---\n",
        "\n",
        "    # Proceed only if collection reset was successful\n",
        "    if collection_reset:\n",
        "        # Check if nodes exist from Cell 5\n",
        "        if 'all_nodes' in locals() and all_nodes:\n",
        "            print(f\"Building index from {len(all_nodes)} nodes into fresh Qdrant collection...\")\n",
        "            try:\n",
        "                # Build the index using the nodes and storage context\n",
        "                index = VectorStoreIndex(\n",
        "                    nodes=all_nodes,\n",
        "                    storage_context=storage_context,\n",
        "                    embed_model=embed_model,\n",
        "                    show_progress=True\n",
        "                 )\n",
        "                print(f\"Index built successfully in Qdrant collection: '{qdrant_collection_name}'.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error building index after collection reset: {e}\")\n",
        "        else:\n",
        "             print(\"Variable 'all_nodes' is empty or missing. Cannot build index.\")\n",
        "             print(\"Ensure Cell 5 ran correctly and produced nodes.\")\n",
        "    else:\n",
        "        print(\"Collection reset failed or was skipped. Index building skipped.\")\n",
        "\n",
        "else:\n",
        "    print(\"Skipping index creation - Prerequisites not met.\")\n",
        "\n",
        "# Final status check\n",
        "if index:\n",
        "    print(\"\\nIndex object is ready.\")\n",
        "else:\n",
        "    print(\"\\nIndex object could not be created. Querying will fail.\")"
      ],
      "metadata": {
        "id": "8aEp6eXsqhvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 9: Setup AI Language Model (Groq)\n",
        "\n",
        "This cell initializes the Large Language Model (LLM) that will generate the final answers. We are using the Groq service, which provides very fast access to powerful open-source models like Llama 3. It uses the Groq API key configured in Cell 6."
      ],
      "metadata": {
        "id": "siFMKEhxAKHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Setup AI Language Model (Groq)\n",
        "import os\n",
        "# Ensure necessary LlamaIndex imports are available\n",
        "from llama_index.llms.groq import Groq\n",
        "from llama_index.core.settings import Settings\n",
        "\n",
        "# --- Initialize Groq LLM ---\n",
        "# Choose a model available on Groq (Llama3 8B is fast and capable)\n",
        "groq_model_name = \"llama3-8b-8192\"\n",
        "llm = None\n",
        "groq_ready = False\n",
        "\n",
        "# Check for the API key set in Cell 6\n",
        "groq_api_key = os.environ.get(\"GROQ_API_KEY\")\n",
        "if groq_api_key:\n",
        "    print(f\"Initializing Groq LLM with model: {groq_model_name}...\")\n",
        "    try:\n",
        "        # Connect to Groq service\n",
        "        llm = Groq(model=groq_model_name, api_key=groq_api_key)\n",
        "        # Set this LLM as the default for LlamaIndex\n",
        "        Settings.llm = llm\n",
        "        print(\"Groq LLM initialized and set globally in Settings.\")\n",
        "        groq_ready = True\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing Groq LLM: {e}\")\n",
        "        print(\"Please ensure your GROQ_API_KEY secret is correct and valid.\")\n",
        "else:\n",
        "    print(\"GROQ_API_KEY not found in environment variables. Cannot initialize Groq LLM.\")\n",
        "\n",
        "if groq_ready:\n",
        "    print(\"LLM configuration successful.\")\n",
        "else:\n",
        "    print(\"LLM configuration failed. Querying will likely fail.\")"
      ],
      "metadata": {
        "id": "dnJ9fGAxqk5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 10: Setup Reranker (Optimization)\n",
        "\n",
        "This cell sets up the reranker, which is an optimization technique. After the initial search retrieves relevant text chunks, the reranker analyzes these chunks specifically against the user's query and re-orders them by relevance. This helps ensure the most important information is passed to the LLM, improving answer quality.\n",
        "\n"
      ],
      "metadata": {
        "id": "ruTSsTWGAO17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Setup Reranker (Optimization)\n",
        "# Ensure necessary LlamaIndex imports are available\n",
        "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
        "\n",
        "# --- Initialize Reranker ---\n",
        "# Uses a cross-encoder model optimized for relevance ranking\n",
        "reranker_model_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
        "# How many top chunks to return after reranking (usually 2-5)\n",
        "rerank_top_n = 3\n",
        "reranker = None\n",
        "reranker_ready = False\n",
        "\n",
        "print(f\"Initializing SentenceTransformerRerank with model: {reranker_model_name}...\")\n",
        "try:\n",
        "    reranker = SentenceTransformerRerank(\n",
        "        top_n=rerank_top_n,\n",
        "        model=reranker_model_name\n",
        "    )\n",
        "    print(f\"Reranker initialized successfully. Will return top {rerank_top_n} nodes.\")\n",
        "    reranker_ready = True\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing reranker: {e}\")\n",
        "    print(\"Check if model name is correct and 'sentence-transformers' library is installed.\")\n",
        "\n",
        "if not reranker_ready:\n",
        "    print(\"Reranker setup failed.\")"
      ],
      "metadata": {
        "id": "x28OIUmcn1F5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 11: Setup Query Engine\n",
        "\n",
        "This cell assembles the final question-answering engine. It combines the indexed data (Cell 8), the mechanism for retrieving relevant chunks (retriever), the reranker for optimization (Cell 10), and the AI language model for generating answers (Cell 9)."
      ],
      "metadata": {
        "id": "9hNQbnwlAeE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Setup Query Engine\n",
        "# Ensure necessary LlamaIndex imports are available\n",
        "from llama_index.core import get_response_synthesizer\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.settings import Settings\n",
        "\n",
        "query_engine = None # Initialize\n",
        "\n",
        "# Check if index, LLM, and reranker are ready\n",
        "if ('index' in locals() and index and\n",
        "    Settings.llm and\n",
        "    'reranker_ready' in locals() and reranker_ready and\n",
        "    'reranker' in locals() and reranker):\n",
        "\n",
        "    print(\"Setting up the RAG query engine...\")\n",
        "    # How many chunks to retrieve initially before reranking\n",
        "    initial_retrieval_k = 10\n",
        "    print(f\"- Retriever will fetch top {initial_retrieval_k} initial results.\")\n",
        "\n",
        "    # Setup the retriever component\n",
        "    retriever = VectorIndexRetriever(\n",
        "        index=index,\n",
        "        similarity_top_k=initial_retrieval_k,\n",
        "    )\n",
        "\n",
        "    # Setup the response synthesizer (uses the LLM)\n",
        "    response_synthesizer = get_response_synthesizer(llm=Settings.llm)\n",
        "    print(f\"- Response synthesizer configured with LLM: {Settings.llm.model}\")\n",
        "\n",
        "    # Define the reranker as a postprocessor\n",
        "    node_postprocessors = [reranker]\n",
        "    print(f\"- Reranker configured: {reranker.model} (will return top {reranker.top_n})\")\n",
        "\n",
        "    # Assemble the final query engine\n",
        "    query_engine = RetrieverQueryEngine(\n",
        "        retriever=retriever,\n",
        "        response_synthesizer=response_synthesizer,\n",
        "        node_postprocessors=node_postprocessors,\n",
        "    )\n",
        "    print(\"\\nQuery engine setup complete and ready for questions.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping query engine setup - Prerequisites not met.\")\n",
        "    if not ('index' in locals() and index): print(\"- Index missing or not loaded (Check Cell 8).\")\n",
        "    if not Settings.llm: print(\"- LLM not configured in Settings (Check Cell 9).\")\n",
        "    if not ('reranker_ready' in locals() and reranker_ready): print(\"- Reranker not ready (Check Cell 10).\")"
      ],
      "metadata": {
        "id": "t5fCr4Pfn13-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 12: Ask Questions (\"Lucky\" Interface)\n",
        "\n",
        "This is the main interaction cell where you can ask questions about the documents you uploaded. The system (\"Lucky\") will use the indexed information and the AI model to provide answers. Type your questions in the input box that appears. Type quit to exit. Including the word source in your query will show the specific text chunks Lucky used to generate the answer."
      ],
      "metadata": {
        "id": "O2IQy892AjZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Ask Questions (\"Lucky\" Interface)\n",
        "import time\n",
        "# Ensure necessary imports are available\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Check if the query engine is ready\n",
        "if 'query_engine' in locals() and query_engine:\n",
        "    display_welcome = True # Flag to show welcome message once\n",
        "\n",
        "    # --- Prepare Context Files String ---\n",
        "    context_files_str = \"N/A (Cell 3 or 5 may not have run correctly)\"\n",
        "    if 'pdf_filenames' in locals() and pdf_filenames:\n",
        "        context_files_str = \", \".join([f\"`{name}`\" for name in pdf_filenames])\n",
        "        if not context_files_str: context_files_str = \"No files processed in current session.\"\n",
        "    # --- End Prepare Context ---\n",
        "\n",
        "    # --- Optional: Predefined Sample Queries ---\n",
        "    # Add 3-5 diverse example questions here for submission/demonstration\n",
        "    test_queries = [\n",
        "        \"What is the property address mentioned in the appraisal report?\",\n",
        "        \"What is the 'Net Pay' on the sample payslip?\",\n",
        "        \"Summarize the 'Term and Termination' section of the sample contract.\",\n",
        "    ]\n",
        "    # Set to True if you want to run the predefined queries first\n",
        "    run_predefined_queries = False\n",
        "    if run_predefined_queries and test_queries:\n",
        "      print(\"\\n--- Running Predefined Sample Queries ---\")\n",
        "      print(f\"(Using context from files: {context_files_str})\")\n",
        "      for query in test_queries:\n",
        "          print(f\"\\nâ“ Query: {query}\")\n",
        "          try:\n",
        "              response = query_engine.query(query)\n",
        "              print(\"\\nâœ… Response:\")\n",
        "              print(response.response)\n",
        "              print(\"-\" * 30)\n",
        "          except Exception as e:\n",
        "              print(f\"\\nâŒ Error processing query: {e}\")\n",
        "      print(\"\\n--- Predefined Queries Finished ---\")\n",
        "      display_welcome = True # Reset flag if predefined runs\n",
        "\n",
        "    # --- Interactive Query Loop ---\n",
        "    if display_welcome:\n",
        "        display(Markdown(\"## âœ¨ Welcome to Lucky ğŸ€, Your Query Assistant! âœ¨\"))\n",
        "        display(Markdown(\"*(Powered by ğŸ€)*\"))\n",
        "        display(Markdown(\"---\"))\n",
        "        display_welcome = False\n",
        "\n",
        "    print(\"\\n--- Starting Interactive Query Session ---\")\n",
        "    print(\"Type your question and press Enter. Type 'quit' to exit.\")\n",
        "    print(\"(Hint: Include the word 'source' in your query to see where Lucky looked!)\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_query = input(\"\\nEnter your query (or type 'quit' to exit): \")\n",
        "        except EOFError:\n",
        "            print(\"\\nInput interrupted. Exiting interactive session.\")\n",
        "            break\n",
        "\n",
        "        if user_query.lower().strip() == 'quit':\n",
        "            print(\"\\nğŸ€ Farewell from Lucky! ğŸˆâ€â¬›\")\n",
        "            break\n",
        "\n",
        "        if not user_query: continue\n",
        "\n",
        "        show_sources = \"source\" in user_query.lower()\n",
        "\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            response = query_engine.query(user_query)\n",
        "            end_time = time.time()\n",
        "\n",
        "            # Build \"Lucky\" Markdown Output\n",
        "            output_md = f\"ğŸ«µğŸ¼ **You Asked:**\\n> {user_query}\\n\\n\"\n",
        "            output_md += f\"ğŸ“‚ **Querying Based On File(s):** {context_files_str}\\n\\n\"\n",
        "            output_md += f\"â³ **Answered in:** {end_time - start_time:.2f} seconds\\n\\n\"\n",
        "            output_md += f\"## ğŸ§  Lucky's Thoughts (Response)\\n\"\n",
        "\n",
        "            llm_response_text = \"(Sorry, Lucky couldn't generate a response ğŸ™‡ğŸ½â€â™‚ï¸)\"\n",
        "            if response and hasattr(response, 'response') and isinstance(response.response, str):\n",
        "                llm_response_text = response.response\n",
        "            elif response and hasattr(response, 'response'):\n",
        "                 llm_response_text = f\"(Response format issue: {str(response.response)} ğŸ™‡ğŸ½â€â™‚ï¸)\"\n",
        "            else:\n",
        "                 llm_response_text = \"(No response object found)\"\n",
        "            output_md += f\"```text\\n{llm_response_text}\\n```\\n\\n\"\n",
        "\n",
        "            if show_sources:\n",
        "                output_md += f\"## ğŸ‘¨ğŸ½â€ğŸ¦¯ Where Lucky Looked (Sources)\\n\"\n",
        "                output_md += f\"Lucky searched through the indexed documents:\\n\\n\"\n",
        "                if response and hasattr(response, 'source_nodes') and response.source_nodes:\n",
        "                     for i, source_node in enumerate(response.source_nodes):\n",
        "                        score_str = f\"{source_node.score:.4f}\" if hasattr(source_node, 'score') and source_node.score is not None else \"N/A\"\n",
        "                        output_md += f\"* **ğŸ“„ Source {i+1}** (Score: {score_str})\\n\"\n",
        "                        output_md += f\"    * **File:** `{source_node.metadata.get('file_name', 'N/A')}`\\n\"\n",
        "                        content_snippet = source_node.get_content()[:300] if hasattr(source_node, 'get_content') else \"(Error retrieving content)\"\n",
        "                        output_md += f\"    * **Content Snippet:**\\n        ```text\\n        {content_snippet}...\\n        ```\\n\"\n",
        "                else:\n",
        "                    output_md += \"* No specific source documents were retrieved or strongly used for this response.\\n\"\n",
        "\n",
        "            output_md += f\"\\n---\\n\"\n",
        "            output_md += f\"ğŸ—£ï¸ **What would you like to ask Lucky next?** (Type `quit` to exit)\\n\"\n",
        "\n",
        "            display(Markdown(output_md))\n",
        "\n",
        "        except Exception as e:\n",
        "            error_md = f\"## Query:\\n> {user_query}\\n\\n**âŒ Sorry, Lucky encountered an error ğŸ™‡ğŸ½â€â™‚ï¸:**\\n```\\n{e}\\n```\"\n",
        "            display(Markdown(error_md))\n",
        "\n",
        "else:\n",
        "    print(\"\\nQuery engine not available. Please run Cell 11 successfully first.\")"
      ],
      "metadata": {
        "id": "fD6ezR95n17d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}